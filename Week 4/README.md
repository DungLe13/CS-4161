## Week 4 Journal

I have been familiar with Slack and GitHub for more than two years. I have gone through all the frustrations with GitHub, ranging from missing files in final submission to accidentally delete some files. However, all in all, GitHub is still one of the most popular version control system that has been used intensively in the tech industry. So I think this is a good opportunity to learn and explore new technologies. That being said, this whole conversation about student materials management system is reaching the point where it is becoming redundant; I was upset that a what should be a 30-minute conversation became out of hand, without any consensus to what should be used. 

Coming into this class, I was also familiar with p5js from a digital art class that I took during my second year. As a result, the whole p5js tutorials acted as a reminder of the concepts and how to draw basic shapes/features. Also, this library is very similar to Processing, thus, I proceeded with no roadblock. I completed all four tutorials on the p5js website; I also played around with the library and draw some animations of the bezier line. In addition, I extended the work related to p5.js and audio visualization to the following weeks. 

A pitch deck for my final project:

**Introduction**

* Dung Le - senior at Bennington College
* Interested in Natural Language Processing and Machine Learning

**Team**

* I will work individually on this project
* Helps are welcomed - especially in the areas concerning CNN and reinforcement learning (deep Q-network)

**Problem**

* Automated Lip Reading (Audio Visual Recognition)
* Assist people with hearing impair; Improve speech recognition tasks, especially in the noisy environment or disrupted audio file.
* Very challenging, due to the variances in input (skin color, mouth shape, accent, speed, etc.) and the one-to-many relationship between viseme and phoneme.

**Advantages**

* Most of the work done relied on either Hidden Markov Model (HMM) or neural network model (CNN and RNN)
* Chung et al. (2016) proposed using dual attention mechanism in addition to LSTMs
* My proposed method utilises reinforcement learning technique - the agent, i.e. my network model, learned the lip reading features by interacting with the environment (sequence of images extracted from close-up video) and maximize the reward achieved by predicting the correct word at any time step.
